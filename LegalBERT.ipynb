{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmQsUGD7xlSy",
        "outputId": "74cfc6de-0546-46eb-a1fe-a56379066039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Comment this when in local runtime\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "9N4ZuoSnzTmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHZ77WffIjrg"
      },
      "outputs": [],
      "source": [
        "! pip install transformers[torch] datasets evaluate\n",
        "\n",
        "# !pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoPVklYkifmW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# # Drive Path\n",
        "Xtrain = np.load('/content/drive/MyDrive/COLIEE data/data/Xtrain_For_H30.npy')\n",
        "Ytrain = np.load('/content/drive/MyDrive/COLIEE data/data/Ytrain_For_H30.npy')\n",
        "\n",
        "# # Local Path  D:\\ResearchPhD\\COLIEE\\data\\data\n",
        "# Xtrain = np.load('/mnt/d/ResearchPhD/COLIEE/data/data/Xtrain_For_H30.npy')\n",
        "# Ytrain = np.load('/mnt/d/ResearchPhD/COLIEE/data/data/Ytrain_For_H30.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VO3e3lNVjsRc"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "# d={}\n",
        "# d[\"train\"]={}\n",
        "\n",
        "# d[\"train\"][\"text\"]=[]\n",
        "# d[\"train\"][\"labels\"]=[]\n",
        "# for i in range(0,len(X_train)):\n",
        "#   d[\"train\"][\"text\"].append(\"<sent-1> \"+str(X_train[i][0], encoding='utf-8')+\"<sent-1>\"+\" <sent-2> \"+str(X_train[i][1], encoding='utf-8')+\"<sent-2>\")\n",
        "#   d[\"train\"][\"labels\"].append(y_train[i])\n",
        "\n",
        "\n",
        "\n",
        "# d[\"test\"]={}\n",
        "\n",
        "# d[\"test\"][\"text\"]=[]\n",
        "# d[\"test\"][\"labels\"]=[]\n",
        "# for i in range(0,len(X_val)):\n",
        "#   d[\"test\"][\"text\"].append(\"<sent-1> \"+str(X_val[i][0], encoding='utf-8')+\"<sent-1>\"+\" <sent-2> \"+str(X_val[i][1], encoding='utf-8')+\"<sent-2>\")\n",
        "#   d[\"train\"][\"labels\"].append(y_val[i])\n",
        "\n",
        "\n",
        "# ds = Dataset.from_dict(d)\n",
        "\n",
        "\n",
        "d={}\n",
        "\n",
        "d[\"text1\"]=[]\n",
        "d[\"text2\"]=[]\n",
        "d[\"label\"]=[]\n",
        "for i in range(0,len(Xtrain)):\n",
        "  d[\"text1\"].append(str(Xtrain[i][0], encoding='utf-8').lower())\n",
        "  d[\"text2\"].append(str(Xtrain[i][1], encoding='utf-8').lower())\n",
        "  d[\"label\"].append(0 if Ytrain[i]=='N' else 1)\n",
        "\n",
        "ds = Dataset.from_dict(d)\n",
        "\n",
        "# d={}\n",
        "\n",
        "# d[\"text\"]=[]\n",
        "# d[\"labels\"]=[]\n",
        "# for i in range(0,len(X_val)):\n",
        "#   d[\"text\"].append(\"<sent-1> \"+str(X_val[i][0], encoding='utf-8')+\"<sent-1>\"+\" <sent-2> \"+str(X_val[i][1], encoding='utf-8')+\"<sent-2>\")\n",
        "#   d[\"labels\"].append(y_val[i])\n",
        "\n",
        "# ds2 = Dataset.from_dict(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLXWhcAY0ugn"
      },
      "outputs": [],
      "source": [
        "train_test = ds.train_test_split(test_size=0.1)\n",
        "# train_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFvDNM5aUUSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f434abbd-a354-4604-ddee-1abc40665908"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text1', 'text2', 'label'],\n",
              "        num_rows: 896\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text1', 'text2', 'label'],\n",
              "        num_rows: 100\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv7MmvlbUMSN"
      },
      "outputs": [],
      "source": [
        "train_test = ds.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aINAdJHtUVNo"
      },
      "outputs": [],
      "source": [
        "testing_set = train_test['test']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_set= train_test['train']\n",
        "\n"
      ],
      "metadata": {
        "id": "SxPfplqlcOB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# minillm download --model llama-7b-4bit --weights llama-7b-4bit.pt"
      ],
      "metadata": {
        "id": "qSs37NJnJBjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import transformers\n",
        "# import torch\n",
        "# tokenizer = transformers.LlamaTokenizer.from_pretrained('chaoyi-wu/PMC_LLAMA_7B')\n",
        "# model = transformers.LlamaForCausalLM.from_pretrained('chaoyi-wu/PMC_LLAMA_7B')\n",
        "# sentence = 'Hello, doctor'\n",
        "# batch = tokenizer(\n",
        "#             sentence,\n",
        "#             return_tensors=\"pt\",\n",
        "#             add_special_tokens=False\n",
        "#         )"
      ],
      "metadata": {
        "id": "Eoa5VzRlA3H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "\n",
        "## v2 models\n",
        "model_path = 'openlm-research/open_llama_3b_v2'\n",
        "# model_path = 'openlm-research/open_llama_7b_v2'\n",
        "\n",
        "## v1 models\n",
        "# model_path = 'openlm-research/open_llama_3b'\n",
        "# model_path = 'openlm-research/open_llama_7b'\n",
        "# model_path = 'openlm-research/open_llama_13b'\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
        "# model = LlamaForCausalLM.from_pretrained(\n",
        "#     model_path, torch_dtype=torch.float16, device_map='cuda'\n",
        "# )\n",
        "\n",
        "# prompt = 'Q: What is the largest animal?\\nA:'\n",
        "# input_ids = tokenizer(prompt, return_tensors=\"pt\", device_map='cuda').input_ids.to('cuda')\n",
        "\n",
        "# generation_output = model.generate(\n",
        "#     input_ids=input_ids, max_new_tokens=32\n",
        "# )\n",
        "# print(tokenizer.decode(generation_output[0]))"
      ],
      "metadata": {
        "id": "OBgmbLi7ylNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm0POgKbQ6Lp"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class LEGAL_BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LEGAL_BERT, self).__init__()\n",
        "        # self.bert_model_sent1 = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\").to(device)\n",
        "        self.bert_model_sent1 =LlamaForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map='cuda').to(device)\n",
        "        self.bert_model_sent1.requires_grad=False\n",
        "        # self.bert_model_sent2 = AutoModel.from_pretrained(\"nlpaueb/bert-base-uncased-echr\").to(device)\n",
        "        # self.bert_model_sent2.requires_grad=False\n",
        "        self.out = nn.Linear(32000, 1)\n",
        "\n",
        "    def forward(self,ids,mask,token_type_ids1):\n",
        "        # _,s1= self.bert_model_sent1(ids,attention_mask=mask, token_type_ids=token_type_ids1,return_dict=False)\n",
        "        _,s1= self.bert_model_sent1(ids,attention_mask=mask,return_dict=False)\n",
        "        print(s1.shape)\n",
        "        # _,s2= self.bert_model_sent2(ids2,attention_mask=mask2,token_type_ids=token_type_ids2, return_dict=False)\n",
        "        out= self.out(s1)\n",
        "\n",
        "        return out\n",
        "\n",
        "model=LEGAL_BERT()\n",
        "\n",
        "\n",
        "\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#Initialize Optimizer\n",
        "optimizer= optim.Adam(model.parameters(),lr= 1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(output)"
      ],
      "metadata": {
        "id": "NHZY-XeTL_iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.create_model_card"
      ],
      "metadata": {
        "id": "RnkwOMdX_87v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbBC8VsyoALz"
      },
      "outputs": [],
      "source": [
        "# # import transformers\n",
        "# from  transformers import AutoTokenizer,AutoModel\n",
        "# import torch\n",
        "# # from transformers import LlamaTokenizer, LlamaForCausalLM\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/bert-base-uncased-echr\")\n",
        "# # tokenizer = transformers.LlamaTokenizer.from_pretrained('chaoyi-wu/PMC_LLAMA_7B')\n",
        "# # tokenizer = transformers.LlamaTokenizer.from_pretrained('chaoyi-wu/PMC_LLAMA_7B')\n",
        "\n",
        "# # model_path='openlm-research/open_llama_3b_v2'\n",
        "\n",
        "# # tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
        "# # model = LlamaForCausalLM.from_pretrained(\n",
        "# #     model_path, torch_dtype=torch.float16, device_map='auto',\n",
        "# # )\n",
        "# # if tokenizer.pad_token is None:\n",
        "# #     tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "# #     model.resize_token_embeddings(len(tokenizer))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "PeqryhOLCI1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2aHe3WepD54"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "epochs = 10\n",
        "BATCH_SIZE = 4\n",
        "model.to(device)\n",
        "from tqdm import tqdm\n",
        "loss_history = []\n",
        "prev_loss=5000\n",
        "patience=5\n",
        "\n",
        "model.train()\n",
        "\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "for  epoch in range(epochs):\n",
        "  loss_agg = 0.0\n",
        "\n",
        "\n",
        "\n",
        "  for index in range(int(len(training_set)/BATCH_SIZE)):\n",
        "    x_batch = training_set[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "    sent1 = x_batch['text1']\n",
        "    print(x_batch['text1'])\n",
        "    print(x_batch['text1'][0])\n",
        "    print(x_batch['text2'][0])\n",
        "    print([x_batch['text1'][0]+x_batch['text2'][0]])\n",
        "    # +x_batch['text2']\n",
        "    # sent2 = x_batch['text2']\n",
        "    label = x_batch['label']\n",
        "    # ids1,token_type_ids1, mask1, ids2,token_type_ids2, mask2 = [],[],[], [],[],[]\n",
        "    ids1,token_type_ids1, mask1 = [],[],[]\n",
        "    for i in range(len(sent1)):\n",
        "        inputs1 = tokenizer.encode_plus(\n",
        "            sent1[i] ,\n",
        "            None,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_token_type_ids=True,\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "        )\n",
        "\n",
        "        # input_ids = tokenizer(prompt, return_tensors=\"pt\")\n",
        "        ids1.append(inputs1[\"input_ids\"])\n",
        "        token_type_ids1.append(inputs1[\"token_type_ids\"])\n",
        "        mask1.append(inputs1[\"attention_mask\"]    )\n",
        "\n",
        "    #     def preprocess_function(examples):\n",
        "    # return tokenizer(examples[\"sentence\"], padding='max_length', truncation=True, return_token_type_ids=False)\n",
        "        # inputs2 = tokenizer.encode_plus(\n",
        "        #     sent2[i] ,\n",
        "        #     None,\n",
        "        #     pad_to_max_length=True,\n",
        "        #     return_attention_mask=True,\n",
        "        #     max_length=512,\n",
        "        #     truncation=True,\n",
        "        # )\n",
        "        # ids2.append(inputs2[\"input_ids\"])\n",
        "        # token_type_ids2.append(inputs2[\"token_type_ids\"])\n",
        "        # mask2.append(inputs2[\"attention_mask\"]    )\n",
        "    ids1 = torch.tensor(ids1, dtype=torch.long).to(device)\n",
        "    token_type_ids1 = torch.tensor(token_type_ids1, dtype=torch.long).to(device)\n",
        "    mask1 = torch.tensor(mask1, dtype=torch.long).to(device)\n",
        "    # ids2 = torch.tensor(ids2, dtype=torch.long).to(device)\n",
        "\n",
        "    # token_type_ids2 = torch.tensor(token_type_ids2, dtype=torch.long).to(device)\n",
        "    # mask2 = torch.tensor(mask2, dtype=torch.long).to(device)\n",
        "    label = torch.tensor(label, dtype=torch.float32).to(device)\n",
        "    label = label.unsqueeze(1)\n",
        "    out = model(ids1,mask1,token_type_ids1).to(device)\n",
        "    print(out)\n",
        "    print(label)\n",
        "    loss=loss_fn(out,label)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    pred = np.where(out.cpu() >= 0., 1, 0)\n",
        "    num_correct = sum(1 for a, b in zip(pred, label) if a[0] == b[0])\n",
        "    num_samples = pred.shape[0]\n",
        "    accuracy = num_correct/num_samples\n",
        "    loss_agg += loss.item()\n",
        "\n",
        "    #TODO\n",
        "    # Write the validation set accuracy\n",
        "    # Aggregate the loss over all batches\n",
        "    # Track both validation loss and training loss\n",
        "    #How to combine the representations from both sentences\n",
        "    # Save the best model\n",
        "    print(f'Epoch number\" {epoch} Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f} | loss {loss}')\n",
        "\n",
        "  avg_loss=loss_agg/4\n",
        "  if (avg_loss)>prev_loss:\n",
        "    patience-=1\n",
        "    print(\"patience:\"+str(patience))\n",
        "  prev_loss=avg_loss\n",
        "\n",
        "  loss_history.append(avg_loss)\n",
        "\n",
        "  plt.plot(loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfQIC6PW689l"
      },
      "outputs": [],
      "source": [
        "#Test on Validation Set\n",
        "import matplotlib.pyplot as plt\n",
        "# epochs = 10\n",
        "BATCH_SIZE = 4\n",
        "# model.to(device)\n",
        "from tqdm import tqdm\n",
        "\n",
        "# for  epoch in range(epochs):\n",
        "#   loss_agg = 0.0\n",
        "#   loss_history = []\n",
        "#   model.train()\n",
        "model.eval()\n",
        "preds=[]\n",
        "\n",
        "for index in range(int(len(testing_set)/BATCH_SIZE)):\n",
        "  print(index)\n",
        "  x_batch = testing_set[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "  sent1 = x_batch['text1']\n",
        "  sent2 = x_batch['text2']\n",
        "  label = x_batch['label']\n",
        "  ids1,token_type_ids1, mask1, ids2,token_type_ids2, mask2 = [],[],[], [],[],[]\n",
        "  for i in range(len(sent1)):\n",
        "      inputs1 = tokenizer.encode_plus(\n",
        "            sent1[i] ,\n",
        "            None,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "        )\n",
        "      ids1.append(inputs1[\"input_ids\"])\n",
        "      token_type_ids1.append(inputs1[\"token_type_ids\"])\n",
        "      mask1.append(inputs1[\"attention_mask\"]    )\n",
        "      inputs2 = tokenizer.encode_plus(\n",
        "            sent2[i] ,\n",
        "            None,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "        )\n",
        "      ids2.append(inputs2[\"input_ids\"])\n",
        "      token_type_ids2.append(inputs2[\"token_type_ids\"])\n",
        "      mask2.append(inputs2[\"attention_mask\"]    )\n",
        "  ids1 = torch.tensor(ids1, dtype=torch.long).to(device)\n",
        "  token_type_ids1 = torch.tensor(token_type_ids1, dtype=torch.long).to(device)\n",
        "  mask1 = torch.tensor(mask1, dtype=torch.long).to(device)\n",
        "  ids2 = torch.tensor(ids2, dtype=torch.long).to(device)\n",
        "\n",
        "  token_type_ids2 = torch.tensor(token_type_ids2, dtype=torch.long).to(device)\n",
        "  mask2 = torch.tensor(mask2, dtype=torch.long).to(device)\n",
        "  label = torch.tensor(label, dtype=torch.float32).to(device)\n",
        "  label = label.unsqueeze(1)\n",
        "  out = model(ids1,mask1,token_type_ids1, ids2,mask2,token_type_ids2).to(device)\n",
        "  # loss=loss_fn(out,label)\n",
        "  # optimizer.zero_grad()\n",
        "  # loss.backward()\n",
        "  # optimizer.step()\n",
        "  pred = np.where(out.cpu() >= 0., 1, 0)\n",
        "  preds.append(pred)\n",
        "    # num_correct = sum(1 for a, b in zip(pred, label) if a[0] == b[0])\n",
        "    # num_samples = pred.shape[0]\n",
        "    # accuracy = num_correct/num_samples\n",
        "    # loss_agg += loss.item()\n",
        "    # loss_history.append(loss.item())\n",
        "    #TODO\n",
        "    # Write the validation set accuracy\n",
        "    # Aggregate the loss over all batches\n",
        "    # Track both validation loss and training loss\n",
        "    #How to combine the representations from both sentences\n",
        "    # Save the best model\n",
        "  #   print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f} | loss {loss}')\n",
        "\n",
        "  # plt.plot(loss_history)\n",
        "\n",
        "preds2=[i.squeeze() for i in preds]\n",
        "preds2=np.concatenate(preds2).ravel().tolist()\n",
        "len(preds2)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(preds2, testing_set[\"label\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gG_zlQpnQTZb"
      },
      "outputs": [],
      "source": [
        "#This is true test set\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "xval = []\n",
        "yval = []\n",
        "\n",
        "# for fl in ['testset/riteval_H30_en.xml']:  \"G:\\ResearchPhD\\Mi Young\\COLIEE work\\COLIEE data\\Samples\\testset\\TestData_en.xml\"\n",
        "for fl in ['/content/drive/MyDrive/COLIEE data/Samples/testset/TestData_en.xml']:\n",
        "    tree = ET.parse(fl)\n",
        "    root = tree.getroot()\n",
        "    for item in root.findall('pair'):\n",
        "        # yval.append(item.attrib['label'])\n",
        "        yval.append(item.attrib['id'])\n",
        "        sample = []\n",
        "        for vals in item:\n",
        "            if vals.tag == 't1':\n",
        "                sample.append(vals.text.encode('utf8').lower().strip())\n",
        "            elif vals.tag == 't2':\n",
        "                sample.append(vals.text.encode('utf8').lower().strip())\n",
        "        xval.append((sample[0], sample[1]))\n",
        "\n",
        "yval=[]\n",
        "file1 = open('/content/drive/MyDrive/COLIEE data/data/answer-task4', 'r')\n",
        "count = 0\n",
        "\n",
        "while True:\n",
        "    line = file1.readline()\n",
        "    if line== '':\n",
        "        break\n",
        "    line=line.split()[1]\n",
        "    yval.append(line)\n",
        "\n",
        "file1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KCwfKgkrI1l"
      },
      "outputs": [],
      "source": [
        "len(yval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4zcieVASOan"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "d={}\n",
        "\n",
        "d[\"text1\"]=[]\n",
        "d[\"text2\"]=[]\n",
        "d[\"label\"]=[]\n",
        "for i in range(0,len(xval)):\n",
        "  d[\"text1\"].append(str(xval[i][0], encoding='utf-8').lower())\n",
        "  d[\"text2\"].append(str(xval[i][1], encoding='utf-8').lower())\n",
        "  d[\"label\"].append(0 if yval[i]=='N' else 1)\n",
        "\n",
        "finaltestset = Dataset.from_dict(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrU8qu55KEUf"
      },
      "outputs": [],
      "source": [
        "len(finaltestset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jpHT9yAKUVU"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "x_batch = finaltestset[index*BATCH_SIZE:(index+1)*BATCH_SIZE]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fURoOZcBK-iL"
      },
      "outputs": [],
      "source": [
        "len(x_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQ5uYr_QNNWr"
      },
      "outputs": [],
      "source": [
        "#Test on Test Set\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# epochs = 10\n",
        "BATCH_SIZE = 4\n",
        "# model.to(device)\n",
        "from tqdm import tqdm\n",
        "\n",
        "# for  epoch in range(epochs):\n",
        "#   loss_agg = 0.0\n",
        "#   loss_history = []\n",
        "#   model.train()\n",
        "model.eval()\n",
        "preds=[]\n",
        "count=0\n",
        "for index in range(int(len(finaltestset)/BATCH_SIZE)+1):\n",
        "  print(index)\n",
        "  count+=1\n",
        "  if count==26:\n",
        "    x_batch=finaltestset[100:101]\n",
        "  else:\n",
        "    x_batch = finaltestset[index*BATCH_SIZE:(index+1)*BATCH_SIZE]\n",
        "  print(index*BATCH_SIZE,(index+1)*BATCH_SIZE,count)\n",
        "  sent1 = x_batch['text1']\n",
        "  sent2 = x_batch['text2']\n",
        "  label = x_batch['label']\n",
        "  ids1,token_type_ids1, mask1, ids2,token_type_ids2, mask2 = [],[],[], [],[],[]\n",
        "  for i in range(len(sent1)):\n",
        "      inputs1 = tokenizer.encode_plus(\n",
        "            sent1[i] ,\n",
        "            None,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "        )\n",
        "      ids1.append(inputs1[\"input_ids\"])\n",
        "      token_type_ids1.append(inputs1[\"token_type_ids\"])\n",
        "      mask1.append(inputs1[\"attention_mask\"]    )\n",
        "      inputs2 = tokenizer.encode_plus(\n",
        "            sent2[i] ,\n",
        "            None,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "        )\n",
        "      ids2.append(inputs2[\"input_ids\"])\n",
        "      token_type_ids2.append(inputs2[\"token_type_ids\"])\n",
        "      mask2.append(inputs2[\"attention_mask\"]    )\n",
        "  ids1 = torch.tensor(ids1, dtype=torch.long).to(device)\n",
        "  token_type_ids1 = torch.tensor(token_type_ids1, dtype=torch.long).to(device)\n",
        "  mask1 = torch.tensor(mask1, dtype=torch.long).to(device)\n",
        "  ids2 = torch.tensor(ids2, dtype=torch.long).to(device)\n",
        "\n",
        "  token_type_ids2 = torch.tensor(token_type_ids2, dtype=torch.long).to(device)\n",
        "  mask2 = torch.tensor(mask2, dtype=torch.long).to(device)\n",
        "  label = torch.tensor(label, dtype=torch.float32).to(device)\n",
        "  label = label.unsqueeze(1)\n",
        "  out = model(ids1,mask1,token_type_ids1, ids2,mask2,token_type_ids2).to(device)\n",
        "  # loss=loss_fn(out,label)\n",
        "  # optimizer.zero_grad()\n",
        "  # loss.backward()\n",
        "  # optimizer.step()\n",
        "  pred = np.where(out.cpu() >= 0., 1, 0)\n",
        "  preds.append(pred)\n",
        "\n",
        "# preds2=[i.squeeze() for i in preds]\n",
        "# preds2=np.concatenate(preds2).ravel().tolist()\n",
        "# len(preds2)\n",
        "\n",
        "preds2 = []\n",
        "for sub_list in preds:\n",
        "    for item in sub_list:\n",
        "        preds2.append(item)\n",
        "\n",
        "preds3=np.concatenate(preds2).ravel().tolist()\n",
        "preds3, len(preds3)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(np.concatenate(preds2).tolist(), finaltestset[\"label\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BDTp_dUxQ50"
      },
      "outputs": [],
      "source": [
        "preds3=np.concatenate(preds2).ravel().tolist()\n",
        "print(preds3, len(preds3))\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(preds3, finaltestset[\"label\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGXv44XWYccT"
      },
      "outputs": [],
      "source": [
        "len(preds3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgrzpUzTToCy"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/COLIEE samples/legalBERT_echr_predictions.pkl', 'wb') as f:  # open a text file\n",
        "    pickle.dump(preds3, f) # serialize the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpSHR25g3gog"
      },
      "outputs": [],
      "source": [
        "torch.save(model, '/content/drive/MyDrive/COLIEE samples/model_scripted.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O5QtnXRmXWkX"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}